ENV = "InvertedPendulumAdv-v1"
MAX_ADVERSARY_STRENGTH = 5
SEED = 0
DISCOUNT = 0.99
POLICY_NOISE = 0.2
EXPLORE_NOISE = 0.2
NOISE_CLIP = 0.4
POLICY_CLIP = 0.2
POLICY_FREQUENCY = 2
TIMESTEPS_PER_BATCH = 128
N_UPDATES_PER_ITERATION = 10
REWARD_THRESH = 1000
HIDDEN_LAYER_DIM = 256
LR = 3e-4
N_TRAINING = 3000
EVAL_EPISODES = 10
RARL_LOOPS = 75
MAX_STEPS_PER_EPISODE = 1000
NUM_EXPERIMENTS = 5
SAVE_DIR = './pendulum_data/'

# ENV = 'HalfCheetahTorsoAdv-v1'
# MAX_ADVERSARY_STRENGTH = 3
# SEED = 0
# INITIAL_OBSERVATION_STEPS = 25e3
# MAX_REPLAY_BUFFER_SIZE = 1e8
# BATCH_SIZE = 128
# DISCOUNT = 0.99
# GAE_LAMBDA = 0.95
# POLICY_CLIP = 0.2
# POLICY_NOISE = 0.2
# EXPLORE_NOISE = 0.2
# NOISE_CLIP = 0.4
# N = 64
# N_EPOCHS = 10
# POLICY_FREQUENCY = 2
# REWARD_THRESH = 6000
# HIDDEN_LAYER_DIM = 256
# N_TRAINING = 10000
# EVAL_EPISODES = 10
# RARL_LOOPS = 100
# MAX_STEPS_PER_EPISODE = 500
# NUM_EXPERIMENTS = 5
# SAVE_DIR = './halfcheetah_data_torso/'

# HAVENT TESTED THIS YET
# ENV = 'HopperHeelAdv-v1'
# MAX_ADVERSARY_STRENGTH = 3
# SEED = 0
# INITIAL_OBSERVATION_STEPS = 25e3
# MAX_REPLAY_BUFFER_SIZE = 1e8
# BATCH_SIZE = 512
# DISCOUNT = 0.99
# GAE_LAMBDA = 0.95
# POLICY_CLIP = 0.2
# EXPLORE_NOISE = 0.2
# BATCH_SIZE = 64 #PPO paper reccomends
# N = 2048#
# N_EPOCHS = 10#
# REWARD_THRESH = 3800.0
# HIDDEN_LAYER_DIM = 256
# N_TRAINING = 6000
# EVAL_EPISODES = 10
# RARL_LOOPS = 100
# MAX_STEPS_PER_EPISODE = 600
# NUM_EXPERIMENTS = 5
# SAVE_DIR = './hopper_data/'